{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOc0orc332wc2N7vK0N5tew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddecosmo-dev/thread-checker/blob/main/threadCheckerBoundingBoxTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjeulXveQtBu",
        "outputId": "a7c8565f-3245-4fc7-f44b-7f6c54cb826c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset already exists.\n"
          ]
        }
      ],
      "source": [
        "#import libraries and download testing data from kaggle\n",
        "#!kaggle datasets download -d devindecosmo/bolt-only-thread-checker-rev-1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Rescaling\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# You will be prompted to upload your 'kaggle.json' API key file.\n",
        "# This only needs to be done once per session.\n",
        "if not os.path.exists(\"/root/.kaggle/kaggle.json\"):\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your kaggle.json file\")\n",
        "    files.upload()\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# The name of the main folder that gets created after unzipping\n",
        "# Change 'training' if your main folder is named something else\n",
        "dataset_folder = 'images'\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "  print('Dataset not found. Downloading...')\n",
        "  # Replace with the API command you copied from your dataset's Kaggle page\n",
        "  !kaggle datasets download -d sujan97/screws-and-nuts-image\n",
        "\n",
        "  # Replace with the actual name of the downloaded .zip file\n",
        "  !unzip -q screws-and-nuts-image.zip\n",
        "  print('Download and unzip complete.')\n",
        "else:\n",
        "  print('Dataset already exists.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the root dataset directory\n",
        "dataset_root = \"yolo_dataset\"\n",
        "os.makedirs(os.path.join(dataset_root, \"images\", \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(dataset_root, \"images\", \"val\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(dataset_root, \"labels\", \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(dataset_root, \"labels\", \"val\"), exist_ok=True)\n",
        "\n",
        "print(\"Directory structure created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOUbJLIqRDVd",
        "outputId": "44cbe2c1-a319-4402-9bff-f0877c26ba74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory structure created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_coco_to_yolo(json_file, images_dir, labels_dir):\n",
        "    \"\"\"\n",
        "    Converts a single COCO JSON file to YOLO format.\n",
        "\n",
        "    Args:\n",
        "        json_file (str): Path to the COCO JSON file.\n",
        "        images_dir (str): Path to the directory containing the images.\n",
        "        labels_dir (str): Path to the directory to save the YOLO labels.\n",
        "    \"\"\"\n",
        "    print(f\"Starting conversion for {json_file}...\")\n",
        "\n",
        "    with open(json_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    # Create a mapping from image_id to image filename and dimensions\n",
        "    img_map = {img['id']: {'file_name': img['file_name'], 'width': img['width'], 'height': img['height']}\n",
        "               for img in coco_data['images']}\n",
        "\n",
        "    # Create a mapping from category_id to a new, zero-indexed class ID\n",
        "    # Since there are only two classes, we map them to 0 and 1\n",
        "    category_map = {cat['id']: i for i, cat in enumerate(coco_data['categories'])}\n",
        "\n",
        "    # Process annotations\n",
        "    for ann in coco_data['annotations']:\n",
        "        img_info = img_map[ann['image_id']]\n",
        "        img_width = img_info['width']\n",
        "        img_height = img_info['height']\n",
        "\n",
        "        # Get the COCO bounding box: [x_min, y_min, width, height]\n",
        "        bbox_coco = ann['bbox']\n",
        "\n",
        "        # Convert to YOLO format: [class_id, x_center, y_center, width, height]\n",
        "        class_id = category_map[ann['category_id']]\n",
        "\n",
        "        # Calculate normalized coordinates\n",
        "        x_center = (bbox_coco[0] + bbox_coco[2] / 2) / img_width\n",
        "        y_center = (bbox_coco[1] + bbox_coco[3] / 2) / img_height\n",
        "        norm_width = bbox_coco[2] / img_width\n",
        "        norm_height = bbox_coco[3] / img_height\n",
        "\n",
        "        # Get the filename without the extension\n",
        "        file_name_no_ext = os.path.splitext(img_info['file_name'])[0]\n",
        "\n",
        "        # Write the annotation to a YOLO .txt file\n",
        "        with open(os.path.join(labels_dir, f'{file_name_no_ext}.txt'), 'a') as f:\n",
        "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\")\n",
        "\n",
        "    print(f\"Conversion complete for {json_file}. {len(coco_data['annotations'])} annotations processed.\")\n",
        "\n",
        "# --- Run the conversion for your training and validation data ---\n",
        "\n",
        "# Paths to your local dataset files\n",
        "local_dataset_path = \"/content/\" # Update this if needed\n",
        "train_json_file = os.path.join(local_dataset_path, \"mvtec_screws_train.json\")\n",
        "val_json_file = os.path.join(local_dataset_path, \"mvtec_screws_val.json\")\n",
        "\n",
        "# Run the function for both splits\n",
        "convert_coco_to_yolo(train_json_file, os.path.join(local_dataset_path, \"images\"), os.path.join(dataset_root ,\"labels\", \"train\"))\n",
        "convert_coco_to_yolo(val_json_file, os.path.join(local_dataset_path, \"images\"), os.path.join(dataset_root, \"labels\", \"val\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0VVV2baUYEl",
        "outputId": "a2f5917e-8b90-47b1-81ae-402c7d078aab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversion for /content/mvtec_screws_train.json...\n",
            "Conversion complete for /content/mvtec_screws_train.json. 3119 annotations processed.\n",
            "Starting conversion for /content/mvtec_screws_val.json...\n",
            "Conversion complete for /content/mvtec_screws_val.json. 647 annotations processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "def get_image_filenames_from_json(json_path):\n",
        "    \"\"\"Reads a COCO JSON file and returns a set of all image filenames.\"\"\"\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return {img['file_name'] for img in data['images']}\n",
        "\n",
        "# Define paths\n",
        "local_dataset_path = \"/content/screws_nuts_data\"\n",
        "yolo_dataset_path = \"yolo_dataset\"\n",
        "original_images_dir = os.path.join(local_dataset_path, \"images\")\n",
        "\n",
        "train_json_path = os.path.join(local_dataset_path, \"mvtec_screws_train.json\")\n",
        "val_json_path = os.path.join(local_dataset_path, \"mvtec_screws_val.json\")\n",
        "\n",
        "train_dest_dir = os.path.join(yolo_dataset_path, \"images\", \"train\")\n",
        "val_dest_dir = os.path.join(yolo_dataset_path, \"images\", \"val\")\n",
        "\n",
        "# Get sets of image filenames for each split\n",
        "train_filenames = get_image_filenames_from_json(train_json_path)\n",
        "val_filenames = get_image_filenames_from_json(val_json_path)\n",
        "\n",
        "# Move the image files to their correct directories\n",
        "for filename in os.listdir(original_images_dir):\n",
        "    src_path = os.path.join(original_images_dir, filename)\n",
        "\n",
        "    if filename in train_filenames:\n",
        "        shutil.move(src_path, train_dest_dir)\n",
        "        print(f\"Moved {filename} to training directory.\")\n",
        "    elif filename in val_filenames:\n",
        "        shutil.move(src_path, val_dest_dir)\n",
        "        print(f\"Moved {filename} to validation directory.\")\n",
        "    else:\n",
        "        # This handles the test images or any other images not in the train/val splits\n",
        "        print(f\"Skipping {filename} (not in training or validation set).\")\n",
        "\n",
        "print(\"\\nImage reorganization complete.\")"
      ],
      "metadata": {
        "id": "Q-amNym1V8yC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}