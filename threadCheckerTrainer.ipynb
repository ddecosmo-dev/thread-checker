{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZKLHx1WZ39TTMR01ocx6TXgXo_xYBaHD",
      "authorship_tag": "ABX9TyN3evsdu1cTadSrE1YmvR/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddecosmo-dev/thread-checker/blob/main/threadCheckerTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries and download testing data from kaggle\n",
        "#!kaggle datasets download -d devindecosmo/bolt-only-thread-checker-rev-1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Rescaling\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# You will be prompted to upload your 'kaggle.json' API key file.\n",
        "# This only needs to be done once per session.\n",
        "if not os.path.exists(\"/root/.kaggle/kaggle.json\"):\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your kaggle.json file\")\n",
        "    files.upload()\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# The name of the main folder that gets created after unzipping\n",
        "# Change 'training' if your main folder is named something else\n",
        "dataset_folder = 'Thread Checker Rev 2, bolts only'\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "  print('Dataset not found. Downloading...')\n",
        "  # Replace with the API command you copied from your dataset's Kaggle page\n",
        "  !kaggle datasets download -d devindecosmo/thread-checker-bolts-rev2\n",
        "\n",
        "  # Replace with the actual name of the downloaded .zip file\n",
        "  !unzip -q thread-checker-bolts-rev2.zip\n",
        "  print('Download and unzip complete.')\n",
        "else:\n",
        "  print('Dataset already exists.')"
      ],
      "metadata": {
        "id": "NjvXC19GJW7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constants\n",
        "#data set creation\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 123;\n",
        "\n",
        "#Data preparation\n",
        "BATCH_SIZE = 16\n",
        "#standard is 32\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "#Model\n",
        "\n",
        "#Training\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "# Define the correct paths to your nested data directories\n",
        "# Note: This assumes your unzipped folder is named 'Thread Checker Rev 1, bolts only'\n",
        "TRAIN_DIR = 'Thread Checker Rev 2, bolts only/training'"
      ],
      "metadata": {
        "id": "GW2iBOiCQDgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing bounding box model training\n",
        "#find a bolt dataset\n",
        "#train a model to make bounding boxes, and save model, may need to be seperate\n",
        "#Apply the bounding boxes to each training photo and resave them\n",
        "#continue as normal\n",
        "\n",
        "#Easy solution first\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def crop_to_center_square(image_path, target_size=224):\n",
        "    \"\"\"\n",
        "    Crops an image to its largest possible central square, then resizes it\n",
        "    to a target square dimension, overwriting the original file.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file to be processed.\n",
        "        target_size (int): The final desired square dimension (e.g., 224).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening image {image_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    width, height = img.size\n",
        "    shortest_side = min(width, height)\n",
        "\n",
        "    left = (width - shortest_side) / 2\n",
        "    top = (height - shortest_side) / 2\n",
        "    right = (width + shortest_side) / 2\n",
        "    bottom = (height + shortest_side) / 2\n",
        "\n",
        "    img_cropped = img.crop((left, top, right, bottom))\n",
        "    img_resized = img_cropped.resize((target_size, target_size), Image.LANCZOS)\n",
        "\n",
        "    # Overwrite the original file\n",
        "    img_resized.save(image_path)\n",
        "    print(f\"Processed and overwrote: {image_path}\")\n",
        "\n",
        "def process_directory(directory_path):\n",
        "    \"\"\"\n",
        "    Iterates through all image files in a directory and overwrites them\n",
        "    with a center-square cropped and resized version.\n",
        "\n",
        "    Args:\n",
        "        directory_path (str): The path to the directory containing images.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(directory_path):\n",
        "        print(f\"Error: Directory not found at {directory_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Starting to process images in directory: {directory_path}\")\n",
        "\n",
        "    processed_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Check if the file is an image\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            crop_to_center_square(file_path)\n",
        "            processed_count += 1\n",
        "        else:\n",
        "            skipped_count += 1\n",
        "            print(f\"Skipping non-image file: {filename}\")\n",
        "\n",
        "    print(f\"\\nProcessing complete. Processed {processed_count} images and skipped {skipped_count} files.\")\n",
        "\n",
        "# --- Example Usage ---\n",
        "# Use this on your training data directory\n",
        "# Make sure to back up your original images first, as this operation is irreversible!\n",
        "train_images_directory = TRAIN_DIR\n",
        "process_directory(train_images_directory)"
      ],
      "metadata": {
        "id": "E02g06hmL7Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset import and creation\n",
        "#may need some scaling or bounding box depending on accuracy\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  TRAIN_DIR,\n",
        "  validation_split=VALIDATION_SPLIT,\n",
        "  subset=\"training\",\n",
        "  seed=123,  # The seed ensures the random split is the same every time\n",
        "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "  batch_size=BATCH_SIZE,\n",
        "  label_mode='categorical') # Add this line to get one-hot encoded labels\n",
        "\n",
        "# We create the validation set from the remaining 20%\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  TRAIN_DIR,\n",
        "  validation_split=VALIDATION_SPLIT,\n",
        "  subset=\"validation\",\n",
        "  seed=123, # Using the same seed is crucial to prevent overlap\n",
        "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "  batch_size=BATCH_SIZE,\n",
        "  label_mode='categorical') # Add this line to get one-hot encoded labels\n",
        "\n",
        "\n",
        "# Get the class names from the folder structure\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Found the following classes:\", class_names)\n",
        "\n",
        "\"\"\"\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "\n",
        "    # Use the label directly as the index\n",
        "    label_index = labels[i]\n",
        "\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[label_index])\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ltDnYzYW6vV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. CONFIGURE DATASETS FOR PERFORMANCE ---\n",
        "# These steps optimize the data pipeline for speed during training.\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"\\nDatasets are loaded, prepared, and ready for training.\")"
      ],
      "metadata": {
        "id": "h7ycDHak9fiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preparation\n",
        "\n",
        "#resizing\n",
        "\n",
        "#normalizing\n",
        "\n",
        "#batching"
      ],
      "metadata": {
        "id": "NUf7UHA8N9IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature engineering"
      ],
      "metadata": {
        "id": "2S60fcB9ONUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model selection and training\n",
        "# --- 4. PREPARE THE MODEL (Transfer Learning) ---\n",
        "\n",
        "# Create a data augmentation layer to prevent overfitting and improve generalization.\n",
        "#randomly changes the postion and orientation to improve training\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "  tf.keras.layers.RandomZoom(0.2),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "# Get the specific preprocessing function required for MobileV2\n",
        "#is this normalization\n",
        "#preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "# Load the pre-trained base model from Keras.\n",
        "# We freeze its weights so we only train our new final layers.\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "                                               include_top=False, # Don't include the final ImageNet classifier\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False # Freeze the convolutional base\n",
        "\n",
        "# Create our own classification head to place on top of the base model\n",
        "inputs = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "x = data_augmentation(inputs)      # 1. Apply augmentation\n",
        "x = Rescaling(1./127.5, offset=-1)(x)            # 2. Preprocess for MobileNetV2\n",
        "x = base_model(x, training=False)  # 3. Run the base model\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x) # 4. Pool features to a single vector, final layer before final\n",
        "x = tf.keras.layers.Dropout(0.2)(x) # 5. Add dropout to prevent overfitting, randomly turns off nuerons to improve operability\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x) # 6. Final prediction layer\n",
        "\n",
        "# Chain all the pieces together into the final model\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Print a summary of the model's architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nxYV__BVOO8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model, configuring it for training\n",
        "#come back and do more research on alternatives, or ways\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled successfully.\")"
      ],
      "metadata": {
        "id": "_roCIMZgBSRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "# Set the number of epochs to train for\n",
        "# Start the training process\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=EPOCHS\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "id": "zqy8fqwVOTLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa685e5e-cb30-41e0-e282-0cba1023a6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.4166 - loss: 1.3022 - val_accuracy: 0.3623 - val_loss: 1.3098\n",
            "Epoch 2/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4697 - loss: 1.0798 - val_accuracy: 0.4348 - val_loss: 1.1267\n",
            "Epoch 3/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.5900 - loss: 0.9576 - val_accuracy: 0.4493 - val_loss: 1.0683\n",
            "Epoch 4/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.5340 - loss: 0.9608 - val_accuracy: 0.5362 - val_loss: 0.9987\n",
            "Epoch 5/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.5898 - loss: 0.9319 - val_accuracy: 0.5362 - val_loss: 0.9543\n",
            "Epoch 6/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.6242 - loss: 0.8449 - val_accuracy: 0.5652 - val_loss: 0.9263\n",
            "Epoch 7/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.6041 - loss: 0.8353 - val_accuracy: 0.6522 - val_loss: 0.7988\n",
            "Epoch 8/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.6166 - loss: 0.9194 - val_accuracy: 0.5942 - val_loss: 0.8225\n",
            "Epoch 9/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.6769 - loss: 0.7750 - val_accuracy: 0.5072 - val_loss: 0.9088\n",
            "Epoch 10/10\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.7040 - loss: 0.7189 - val_accuracy: 0.5942 - val_loss: 0.8304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training metrics\n",
        "# Extract the accuracy history\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Extract the loss history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get the number of epochs\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#additional validation stuff to add\n",
        "#confusion matrices, other parameters"
      ],
      "metadata": {
        "id": "BBrz01-mBx3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving trained model for GUI usage\n",
        "\n",
        "# Define a path in your Google Drive.\n",
        "# '/content/drive/MyDrive/' is the standard path to your main \"My Drive\" folder.\n",
        "model_path = '/content/drive/MyDrive/24-679 AI ML Projects/Project 0/thread_checker_model.keras'\n",
        "\n",
        "# Save the model to the specified path.\n",
        "model.save(model_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")"
      ],
      "metadata": {
        "id": "blpwE16H6A33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save a JSON of directory data for GUI\n",
        "# --- CONFIGURE YOUR PATHS HERE ---\n",
        "DATASET_PATH = '/content/Thread Checker Rev 1, bolts only/training/' # Path to the dataset\n",
        "OUTPUT_FILE_PATH = '/content/drive/MyDrive/24-679 AI ML Projects/Project 0/class_map.json' # Output JSON file\n",
        "\n",
        "# --- SCRIPT TO BUILD THE NESTED MAP ---\n",
        "class_map = {}\n",
        "class_index = 0\n",
        "print(f\"Scanning directory: {DATASET_PATH}\")\n",
        "\n",
        "# os.walk goes through every folder and subfolder\n",
        "for root, dirs, files in os.walk(DATASET_PATH):\n",
        "    # We are looking for the deepest folders which contain the images\n",
        "    if not dirs: # If a folder has no subdirectories, it's a leaf folder\n",
        "        # Get the full path label, like 'bolts/Hex_Bolt_0.25'\n",
        "        full_label = os.path.relpath(root, DATASET_PATH)\n",
        "\n",
        "        # This is the unique key your model will be trained to recognize\n",
        "        # Keras/TF use the full path as the class name in this case\n",
        "        class_map[full_label] = class_index\n",
        "        class_index += 1\n",
        "\n",
        "if not class_map:\n",
        "    print(\"WARNING: No leaf directories found. Did not generate a map.\")\n",
        "else:\n",
        "    # Save the structured dictionary to a JSON file\n",
        "    with open(OUTPUT_FILE_PATH, 'w') as f:\n",
        "        json.dump(class_map, f, indent=2)\n",
        "    print(f\"✅ Successfully created class map with {len(class_map)} entries at {OUTPUT_FILE_PATH}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "90Fg_t9jBA0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##fine tuning if desired?\n",
        "\"\"\"\n",
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Re-compile the model with a very low learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # 10x smaller\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continue training for a few more epochs\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = epochs + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(train_ds,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         validation_data=val_ds)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jUlaYWRS1B2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}